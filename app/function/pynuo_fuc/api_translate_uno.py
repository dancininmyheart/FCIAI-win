# è¯·å…ˆå®‰è£… OpenAI SDK: `pip3 install openai`
"""
api_translate_uno.py
æ”¯æŒæ®µè½å±‚çº§çš„ç¿»è¯‘APIæ¨¡å—
"""
import sys, os

sys.path.insert(0, os.path.dirname(__file__))

import json
import re
import requests  # æ–°å¢ï¼šç”¨äºè°ƒç”¨åç«¯API
from logger_config import get_logger
from openai import OpenAI
import unicodedata
import ast
from typing import List, Dict

# è·å–æ—¥å¿—è®°å½•å™¨
logger = get_logger("pyuno")

QWEN_API_KEY = os.getenv("QWEN_API_KEY")
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")


def translate(
    text: str,
    field: str = "",
    stop_words: List[str] = [],
    custom_translations: Dict[str, str] = {},
    source_language: str = "English",
    target_language: str = "Chinese",
    model: str = "qwen",
):
    # å°†stop_wordså’Œcustom_translationsè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    logger.info(f"translate_api_unoå¼€å§‹å·¥ä½œï¼Œæ‰§è¡Œå°†{source_language}ç¿»è¯‘ä¸º{target_language}çš„ä»»åŠ¡")
    stop_words_str = ", ".join(f'"{word}"' for word in stop_words)
    custom_translations_str = ", ".join(f'"{k}": "{v}"' for k, v in custom_translations.items())
    if model == "qwen":
        logger.info("modelå‚æ•°è®¾ç½®ä¸ºqwen,ä½¿ç”¨qwen2.5-72b-instructæ¨¡å‹")
        client = OpenAI(api_key=QWEN_API_KEY, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        used_model = "qwen3-235b-a22b-instruct-2507"
        response = client.chat.completions.create(
            model=used_model,
            messages=[
                {
                    "role": "system",
                    "content": f"""æ‚¨æ˜¯ç¿»è¯‘{field}é¢†åŸŸæ–‡æœ¬çš„ä¸“å®¶ã€‚æ¥ä¸‹æ¥ï¼Œæ‚¨å°†è·å¾—ä¸€ç³»åˆ—{source_language}æ–‡æœ¬ï¼ˆåŒ…æ‹¬çŸ­è¯­ã€å¥å­å’Œå•è¯ï¼‰ï¼Œä»–ä»¬æ˜¯éš¶å±äºåŒä¸€ä¸ªPPTçš„åŒä¸€é¡µé¢ä¸‹çš„æ–‡æœ¬æ¡†æ®µè½çš„æ‰€æœ‰æ–‡æœ¬ã€‚
                                                  è¯·å°†æ¯ä¸€æ®µæ–‡æœ¬ç¿»è¯‘æˆä¸“ä¸šçš„{target_language}æ–‡æœ¬ã€‚
                                                  1. ä¸Šä¼ çš„å°†æ˜¯ä¸€ä¸ªæ ¼å¼åŒ–æ–‡æœ¬ï¼Œç»“æ„å¦‚ä¸‹ï¼š
                                                    ç¬¬1é¡µå†…å®¹ï¼š

                                                    ã€æ–‡æœ¬æ¡†1-æ®µè½1ã€‘
                                                    ã€æ–‡æœ¬æ¡†1-æ®µè½1å†…çš„åŸå§‹æ–‡æœ¬ã€‘

                                                    ã€æ–‡æœ¬æ¡†1-æ®µè½2ã€‘
                                                    ã€æ–‡æœ¬æ¡†1-æ®µè½2å†…çš„åŸå§‹æ–‡æœ¬ã€‘

                                                    ã€æ–‡æœ¬æ¡†2-æ®µè½1ã€‘
                                                    ã€æ–‡æœ¬æ¡†2-æ®µè½1å†…çš„åŸå§‹æ–‡æœ¬ã€‘
                                                     
                                                    æ¯ä¸€ä¸ªæ–‡æœ¬å…ƒç´ éƒ½æ˜¯è¯¥PPTé¡µé¢å†…ä¸€ä¸ªæ–‡æœ¬æ¡†çš„ä¸€ä¸ªæ®µè½çš„å®Œæ•´å†…å®¹ï¼Œè¯·**ä¿æŒæ•´ä½“æ€§**ï¼Œå³ä¾¿å‡ºç°æ¢è¡Œç¬¦ç­‰ç‰¹æ®Šç¬¦å·ï¼Œä¹ŸåŠ¡å¿…å®Œæ•´ç¿»è¯‘å…¨æ–‡,åŒæ—¶ä¿ç•™è¿™äº›æ¢è¡Œç¬¦ã€‚
                                                  2. åŸæ–‡ä¸­å­˜åœ¨å½¢å¼ä¸º[block]çš„åˆ†éš”ç¬¦ï¼Œè¯¥ç¬¦çš„ä½œç”¨æ˜¯åŒºåˆ†ä¸åŒå­—ä½“æ ¼å¼çš„æ–‡æœ¬ï¼Œè¯·ä¸è¦å¯¹[block]ç¬¦è¿›è¡Œç¿»è¯‘ã€‚ä½†æ˜¯åœ¨ç¿»è¯‘åçš„å†…å®¹ä¸­ä½ ä»ç„¶éœ€è¦åœ¨æœ€åå¤„ç†æ—¶è¦æ’å…¥ä¸åŸæ–‡ç›¸åŒæ•°é‡çš„[block]ç¬¦,ä¸”æ’å…¥çš„ä½ç½®åº”è¯¥åœ¨ä¸åŸæ–‡ç›¸åŒè¯ä¹‰çš„ä½ç½®ï¼Œä»¥æ­¤ä½œä¸ºåç»­æ ¼å¼å¤„ç†çš„æ ‡è®°ã€‚
                                                     ä½ éœ€è¦ä¿è¯ç¿»è¯‘åçš„å†…å®¹ä¸­ï¼Œ[block]ç¬¦çš„ä¸ªæ•°ä¸åŸæ–‡ç›¸åŒï¼Œè¿™ä¹Ÿå°±ä»£è¡¨ç€è¯‘å‰è¯‘åæ‹¥æœ‰ç›¸åŒæ•°é‡çš„æ–‡æœ¬ç‰‡æ®µï¼Œè¿™äº›æ–‡æ®µæœ‰ä¸åŒçš„å­—ä½“æ ¼å¼ï¼Œä½†ä¸€ä¸€å¯¹åº”ã€‚
                                                  3. ä¸è¦è¾“å‡ºä»»ä½•ä¸å¯è§å­—ç¬¦ã€æ§åˆ¶å­—ç¬¦ã€ç‰¹æ®Šç¬¦å·
                                                  4. å¦‚æœåŸæ–‡å‡ºç°äº†ä¸­æ–‡ç”šè‡³å…¨æ–‡æ®µéƒ½æ˜¯ä¸­æ–‡ï¼Œå°±å°†ä¸­æ–‡å†™åœ¨source_languageä¸­ï¼Œä¸”target_languageä¸­ä»ç„¶ä¿ç•™ã€‚
                                                  5. è¾“å‡ºæ ¼å¼åº”ä¸¥æ ¼ä¿æŒè¾“å…¥é¡ºåºï¼Œä¸€æ®µå¯¹åº”ä¸€æ®µï¼Œä½¿ç”¨å¦‚ä¸‹ JSON æ ¼å¼è¾“å‡ºï¼š
                                                  [
                                                      {{
                                                          \"box_index\": 1,
                                                          \"paragraph_index\": 1,
                                                          \"source_language\": \"ã€æ–‡æœ¬æ¡†1-æ®µè½1çš„åŸå§‹æ–‡æœ¬ã€‘\",
                                                          \"target_language\": \"ã€æ–‡æœ¬æ¡†1-æ®µè½1çš„ç¿»è¯‘ã€‘\"
                                                      }},
                                                      {{
                                                          \"box_index\": 1,
                                                          \"paragraph_index\": 2,
                                                          \"source_language\": \"ã€æ–‡æœ¬æ¡†1-æ®µè½2çš„åŸå§‹æ–‡æœ¬ã€‘\",
                                                          \"target_language\": \"ã€æ–‡æœ¬æ¡†1-æ®µè½2çš„ç¿»è¯‘ã€‘\"
                                                      }},
                                                      {{
                                                          \"box_index\": 2,
                                                          \"paragraph_index\": 1,
                                                          \"source_language\": \"ã€æ–‡æœ¬æ¡†2-æ®µè½1çš„åŸå§‹æ–‡æœ¬ã€‘\",
                                                          \"target_language\": \"ã€æ–‡æœ¬æ¡†2-æ®µè½1çš„ç¿»è¯‘ã€‘\"
                                                      }}
                                                  ]
                                                  **é‡è¦ï¼šè¯·ä¸¥æ ¼éµå®ˆä»¥ä¸‹ç¿»è¯‘è§„åˆ™**ï¼š
                                                  1. **æ ¼å¼è¦æ±‚**ï¼š
                                                      - å¯¹æ¯ä¸ªæ–‡æœ¬æ¡†æ®µè½ï¼Œè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
                                                      {{
                                                          \"box_index\": æ–‡æœ¬æ¡†åºå·,
                                                          \"paragraph_index\": æ®µè½åºå·,
                                                          \"source_language\": \"åŸè¯­è¨€æ–‡æœ¬\",
                                                          \"target_language\": \"è¯‘æ–‡\"
                                                      }}
                                                      - æŒ‰æ–‡æœ¬æ¡†æ®µè½é¡ºåºåœ¨ **åŒä¸€ä¸ª JSON æ•°ç»„** å†…è¾“å‡º
                                                      - **ä¸è¦è¾“å‡ºé¢å¤–ä¿¡æ¯ã€æ³¨é‡Šæˆ–å¤šä½™æ–‡æœ¬**ã€‚
                                                      - box_index å’Œ paragraph_index å¿…é¡»ä¸è¾“å…¥ä¸­çš„ã€æ–‡æœ¬æ¡†X-æ®µè½Yã€‘åºå·å®Œå…¨å¯¹åº”
                                                  2. **è‡ªå®šä¹‰ç¿»è¯‘**ï¼š
                                                     å¦‚æœé‡åˆ°ä»¥ä¸‹è¯æ±‡ï¼Œåœ¨ä¿æŒè¯­ä¹‰é€šé¡ºçš„å‰æä¸‹ä½¿ç”¨æä¾›çš„ç¿»è¯‘åšå‚è€ƒï¼š
                                                      {custom_translations_str}
                                                  3. **åœç¿»è¯å¤„ç†**ï¼š
                                                     ä»¥ä¸‹æˆ–å•è¯çŸ­è¯­**ä¿ç•™åŸæ ·ï¼Œä¸ç¿»è¯‘**ï¼š
                                                      {stop_words_str}
                                                  ç°åœ¨ï¼Œè¯·æŒ‰ç…§ä¸Šè¿°è§„åˆ™ç¿»è¯‘æ–‡æœ¬""",
                },
                {"role": "user", "content": text},
            ],
            stream=False,
            max_tokens=32768,
        )
        return response.choices[0].message.content

    elif model == "deepseek":
        logger.info("modelå‚æ•°è®¾ç½®ä¸ºdeepseek,ä½¿ç”¨åç«¯translate_ppt_pageæ¥å£")
        return call_backend_translate_ppt_page(
            text, "deepseek", field, stop_words_str, custom_translations_str, source_language, target_language
        )

    elif model == "gpt4o":
        logger.info("modelå‚æ•°è®¾ç½®ä¸ºgpt4o,ä½¿ç”¨åç«¯translate_ppt_pageæ¥å£")
        return call_backend_translate_ppt_page(
            text, "gpt4o", field, stop_words_str, custom_translations_str, source_language, target_language
        )

    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹: {model}")


def call_backend_translate_ppt_page(
    text, model, field, stop_words_str, custom_translations_str, source_language, target_language, timeout=120
):
    """
    è°ƒç”¨åç«¯çš„translate_ppt_pageæ¥å£

    Args:
        text: è¦ç¿»è¯‘çš„æ–‡æœ¬
        model: æ¨¡å‹ç±»å‹ ("deepseek" æˆ– "gpt4o")
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        str: å¤§æ¨¡å‹çš„åŸå§‹responseï¼ˆç›´æ¥è¿”å›dataï¼‰
    """
    # APIåŸºç¡€åœ°å€å’Œç«¯ç‚¹é…ç½®ï¼ˆä½¿ç”¨api_test.pyä¸­çš„é…ç½®ï¼‰
    base_url = "http://117.50.216.15/agent_server/app/run/"

    # ä½¿ç”¨api_test.pyä¸­çš„ç«¯ç‚¹ID
    endpoints = {"gpt4o": "dd69b399afaf46a18efe751e0f21f05f", "deepseek": "d145ae592efa4240867c3b1f99c7a5d7"}

    if model not in endpoints:
        raise ValueError(f"ä¸æ”¯æŒçš„åç«¯æ¨¡å‹: {model}")

    endpoint_id = endpoints[model]
    url = f"{base_url}{endpoint_id}"

    # æ„å»ºè¯·æ±‚è½½è·
    payload = {
        "_streaming": False,
        "is_app_uid": False,
        "field": field,
        "text": text,
        "stop_words_str": stop_words_str,
        "custom_translations_str": custom_translations_str,
        "source_language": source_language,
        "target_language": target_language,
    }

    headers = {"Content-Type": "application/json", "User-Agent": "Python-API-Client/1.0"}

    try:
        logger.info(f"æ­£åœ¨è°ƒç”¨åç«¯API: {url}")
        logger.debug(f"è¯·æ±‚è½½è·: {json.dumps(payload, ensure_ascii=False, indent=2)}")

        response = requests.post(url, json=payload, headers=headers, timeout=timeout)
        response.raise_for_status()

        result = response.json()
        logger.debug(f"åç«¯APIåŸå§‹å“åº”: {json.dumps(result, ensure_ascii=False, indent=2)}")

        # æ£€æŸ¥å“åº”çŠ¶æ€
        if result.get("code") == 200:
            data = result.get("data", "")
            logger.info(f"åç«¯APIè°ƒç”¨æˆåŠŸï¼Œå¤„ç†è¿”å›æ•°æ®")

            # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†åç«¯è¿”å›çš„æ•°æ®ç»“æ„
            if isinstance(data, dict):
                # å¦‚æœdataæ˜¯å­—å…¸ä¸”åŒ…å«translated_jsonå­—æ®µï¼Œæå–è¯¥å­—æ®µ
                if "translated_json" in data:
                    translated_json = data["translated_json"]
                    logger.info(f"æå–translated_jsonå­—æ®µ")

                    # å¦‚æœtranslated_jsonæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
                    if isinstance(translated_json, str):
                        logger.info(f"translated_jsonæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›")
                        return translated_json

                    # å¦‚æœtranslated_jsonæ˜¯åˆ—è¡¨æˆ–å…¶ä»–ç»“æ„ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
                    else:
                        json_result = json.dumps(translated_json, ensure_ascii=False)
                        logger.info(f"translated_jsonè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼Œé•¿åº¦: {len(json_result)} å­—ç¬¦")
                        return json_result

                # å¦‚æœdataæ˜¯å­—å…¸ä¸”åŒ…å«outputå­—æ®µï¼Œæå–outputå­—æ®µ
                elif "output" in data:
                    output_data = data["output"]

                    # å¦‚æœoutputä¹Ÿæ˜¯å­—å…¸ä¸”åŒ…å«translated_json
                    if isinstance(output_data, dict) and "translated_json" in output_data:
                        translated_json = output_data["translated_json"]
                        if isinstance(translated_json, str):
                            return translated_json
                        else:
                            return json.dumps(translated_json, ensure_ascii=False)

                    # å¦åˆ™ç›´æ¥å¤„ç†output
                    json_result = json.dumps(output_data, ensure_ascii=False)
                    logger.info(f"æå–outputå­—æ®µå¹¶è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼Œé•¿åº¦: {len(json_result)} å­—ç¬¦")
                    return json_result

                # å¦‚æœéƒ½æ²¡æœ‰ç‰¹æ®Šå­—æ®µï¼Œç›´æ¥è½¬æ¢æ•´ä¸ªdata
                else:
                    json_result = json.dumps(data, ensure_ascii=False)
                    logger.info(f"ç›´æ¥è½¬æ¢dataä¸ºJSONå­—ç¬¦ä¸²ï¼Œé•¿åº¦: {len(json_result)} å­—ç¬¦")
                    return json_result

            # å¦‚æœdataå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›
            elif isinstance(data, str):
                logger.info(f"dataå·²ç»æ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›")
                return data

            # å¦‚æœdataæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            elif isinstance(data, list):
                json_result = json.dumps(data, ensure_ascii=False)
                logger.info(f"dataæ˜¯åˆ—è¡¨ï¼Œè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼Œé•¿åº¦: {len(json_result)} å­—ç¬¦")
                return json_result

            # å…¶ä»–æƒ…å†µï¼Œå°è¯•è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
            else:
                try:
                    json_result = json.dumps(data, ensure_ascii=False)
                    logger.info(f"dataè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²ï¼Œé•¿åº¦: {len(json_result)} å­—ç¬¦")
                    return json_result
                except Exception as e:
                    logger.error(f"æ— æ³•å°†dataè½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²: {e}")
                    raise ValueError(f"åç«¯è¿”å›çš„æ•°æ®æ ¼å¼æ— æ³•å¤„ç†: {type(data)}")
        else:
            error_msg = result.get("msg", "æœªçŸ¥é”™è¯¯")
            logger.error(f"åç«¯APIè°ƒç”¨å¤±è´¥: çŠ¶æ€ç  {result.get('code')}, é”™è¯¯ä¿¡æ¯: {error_msg}")
            raise ValueError(f"åç«¯APIè°ƒç”¨å¤±è´¥: {error_msg}")

    except requests.exceptions.Timeout:
        logger.error(f"åç«¯APIè°ƒç”¨è¶…æ—¶ (è¶…è¿‡ {timeout} ç§’)")
        raise ValueError(f"åç«¯APIè°ƒç”¨è¶…æ—¶ï¼Œè¯·ç¨åé‡è¯•")

    except requests.exceptions.ConnectionError as e:
        logger.error(f"åç«¯APIè¿æ¥é”™è¯¯: {e}")
        raise ValueError("æ— æ³•è¿æ¥åˆ°åç«¯APIæœåŠ¡å™¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")

    except requests.exceptions.RequestException as e:
        logger.error(f"åç«¯APIè¯·æ±‚å¼‚å¸¸: {e}")
        raise ValueError(f"åç«¯APIè¯·æ±‚å¤±è´¥: {str(e)}")

    except json.JSONDecodeError as e:
        logger.error(f"åç«¯APIå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {e}")
        raise ValueError("åç«¯APIè¿”å›çš„å“åº”æ ¼å¼æ— æ•ˆ")

    except Exception as e:
        logger.error(f"è°ƒç”¨åç«¯APIæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
        raise ValueError(f"è°ƒç”¨åç«¯APIæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")


def clean_translation_text(text: str) -> str:
    """
    æ¸…ç†ç¿»è¯‘æ–‡æœ¬ä¸­çš„ä¸å¯è§å­—ç¬¦å’Œç‰¹æ®Šæ§åˆ¶å­—ç¬¦
    """
    if not text:
        return text

    # ç§»é™¤å¸¸è§æ§åˆ¶å­—ç¬¦
    text = re.sub(r"[\x00-\x1F\x7F]", "", text)
    # ç§»é™¤é›¶å®½å­—ç¬¦ã€ä¸å¯è§ç©ºæ ¼ç­‰
    invisible_chars = [
        "\u200b",  # é›¶å®½ç©ºæ ¼
        "\u200c",  # é›¶å®½éè¿æ¥ç¬¦
        "\u200d",  # é›¶å®½è¿æ¥ç¬¦
        "\u200e",  # ä»å·¦åˆ°å³æ ‡è®°
        "\u200f",  # ä»å³åˆ°å·¦æ ‡è®°
        "\u202a",  # ä»å·¦åˆ°å³åµŒå…¥
        "\u202b",  # ä»å³åˆ°å·¦åµŒå…¥
        "\u202c",  # åµŒå…¥ç»“æŸ
        "\u202d",  # ä»å·¦åˆ°å³è¦†ç›–
        "\u202e",  # ä»å³åˆ°å·¦è¦†ç›–
        "\ufeff",  # BOM
    ]
    for ch in invisible_chars:
        text = text.replace(ch, "")

    # è¿˜å¯ä»¥ç”¨unicodedataè¿‡æ»¤æ‰€æœ‰ç±»åˆ«ä¸º"Cf"çš„å­—ç¬¦
    text = "".join(c for c in text if unicodedata.category(c) != "Cf")

    return text.strip()


def parse_formatted_text_async(text: str):
    """
    å¼‚æ­¥è§£ææ ¼å¼åŒ–æ–‡æœ¬ï¼ˆJSONï¼‰

    Args:
        text: æ ¼å¼åŒ–æ–‡æœ¬

    Returns:
        è§£æç»“æœ
    """
    logger.debug(f"åŸå§‹å¾…è§£ææ–‡æœ¬: {repr(text)}")
    cleaned_text = clean_translation_text(text)
    logger.debug(f"æ¸…ç†åå¾…è§£ææ–‡æœ¬: {repr(cleaned_text)}")

    # å…ˆå°è¯•ç›´æ¥ç”¨jsonè§£æ
    try:
        return json.loads(cleaned_text)
    except json.JSONDecodeError as e:
        logger.warning(f"åˆæ¬¡è§£æ JSON å¤±è´¥ï¼Œå°è¯• ast.literal_eval: {e}")
        try:
            result = ast.literal_eval(cleaned_text)
            logger.info("ä½¿ç”¨ ast.literal_eval æˆåŠŸè§£æ")
            return result
        except Exception as e2:
            logger.warning(f"ast.literal_eval è§£æå¤±è´¥ï¼Œå°è¯•æ­£åˆ™æå–: {e2}")
            # å°è¯•æ­£åˆ™æå–JSONä¸»ä½“
            json_block = extract_json_block(cleaned_text)
            try:
                return json.loads(json_block)
            except Exception as e3:
                logger.warning(f"æ­£åˆ™æå–åä»å¤±è´¥ï¼Œå°è¯•å¤§æ¨¡å‹ä¿®å¤: {e3}")
                # è°ƒç”¨å¤§æ¨¡å‹ä¿®å¤
                fixed_text = clean_translation_text(re_parse_formatted_text_async(json_block))
                logger.debug(f"ä¿®å¤åå¾…è§£ææ–‡æœ¬: {repr(fixed_text)}")
                return json.loads(fixed_text)


def re_parse_formatted_text_async(text: str):
    """
    åŒæ­¥é‡æ–°è§£ææ ¼å¼åŒ–æ–‡æœ¬ï¼Œä¿®å¤å¯èƒ½çš„æ ¼å¼é”™è¯¯
    Args:
        text: æ ¼å¼å¯èƒ½é”™è¯¯çš„æ–‡æœ¬
    Returns:
        ä¿®å¤åçš„æ–‡æœ¬
    """
    try:
        client = OpenAI(api_key=QWEN_API_KEY, base_url="https://dashscope.aliyuncs.com/compatible-mode/v1")
        response = client.chat.completions.create(
            model="qwen3-235b-a22b-instruct-2507",
            messages=[
                {
                    "role": "system",
                    "content": """
                 ä½ æ˜¯ä¸€ä¸ª JSON è§£æå’Œä¿®å¤ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä¿®å¤ä¸€æ®µ **å¯èƒ½å­˜åœ¨æ ¼å¼é”™è¯¯çš„ JSON**ï¼Œå¹¶è¾“å‡ºä¸€ä¸ª **ä¸¥æ ¼ç¬¦åˆ JSON æ ‡å‡†** çš„ **æ ¼å¼æ­£ç¡®çš„ JSON**ã€‚

### **è§„åˆ™è¦æ±‚ï¼š**
1. **ç¡®ä¿ JSON æ ¼å¼æ­£ç¡®**ï¼šä¿®å¤ä»»ä½•å¯èƒ½çš„è¯­æ³•é”™è¯¯ï¼Œå¦‚ç¼ºå°‘å¼•å·ã€é€—å·ã€æ‹¬å·ä¸åŒ¹é…ç­‰ã€‚
2. **ä¿æŒåŸå§‹ç»“æ„å’Œæ•°æ®**ï¼šé™¤éå¿…è¦ï¼Œå°½é‡ä¸ä¿®æ”¹åŸå§‹æ•°æ®å†…å®¹ï¼Œä»…ä¿®å¤æ ¼å¼é—®é¢˜ã€‚
3. **æ­£ç¡®å¤„ç†æ•°æ®ç±»å‹**ï¼š
   - **å­—ç¬¦ä¸²** åº”è¯¥ä½¿ç”¨ **åŒå¼•å· `"`** åŒ…è£¹ï¼Œè€Œä¸æ˜¯å•å¼•å· `'`ã€‚
   - **æ•°å­—** åº”ä¿æŒåŸå§‹æ•°å€¼ï¼Œä¸è¦è½¬æ¢ä¸ºå­—ç¬¦ä¸²ã€‚
   - **å¸ƒå°”å€¼**ï¼ˆ`true` / `false`ï¼‰å’Œ **null** å¿…é¡»ç¬¦åˆ JSON è§„èŒƒï¼Œä¸è¦è¯¯ä¿®æ”¹ã€‚
4. **ä¸è¾“å‡ºé¢å¤–æ–‡æœ¬**ï¼š
   - **ä»…è¾“å‡ºä¿®å¤åçš„ JSON**ï¼Œä¸è¦æ·»åŠ è§£é‡Šã€æ³¨é‡Šæˆ–é¢å¤–çš„è¯´æ˜æ–‡æœ¬ã€‚
   """,
                },
                {"role": "user", "content": text},
            ],
            temperature=0.3,
            max_tokens=16000,  # Increased from 8000 to 16000 to handle larger JSON responses
        )
        result = response.choices[0].message.content
        logger.info(f"JSONä¿®å¤æˆåŠŸ")
        return result
    except Exception as e:
        logger.error(f"ä¿®å¤JSONæ ¼å¼å¤±è´¥ï¼Œè¿”å›åŸæ–‡: {str(e)}")
        return text


def separate_translate_text(text_translate):
    """
    è§£æç¿»è¯‘åçš„JSONæ–‡æœ¬ï¼Œæå–æ‰€æœ‰target_languageå­—æ®µï¼Œå¹¶æŒ‰æ–‡æœ¬æ¡†æ®µè½ç´¢å¼•ç»„ç»‡
    """
    logger.debug(f"å¼€å§‹è§£æç¿»è¯‘ç»“æœï¼Œè¾“å…¥ç±»å‹: {type(text_translate)}")
    logger.debug(f"è¾“å…¥å†…å®¹å‰100å­—ç¬¦: {str(text_translate)[:100]}...")

    # å¯¹jsonæ–‡æœ¬è¿›è¡Œç®€å•çš„å­—ç¬¦è¿‡æ»¤
    text_clean = clean_translation_text(text_translate)
    logger.debug(f"æ¸…ç†åæ–‡æœ¬ç±»å‹: {type(text_clean)}")
    logger.debug(f"æ¸…ç†åå†…å®¹å‰100å­—ç¬¦: {str(text_clean)[:100]}...")

    # è§£æJSON
    try:
        data = parse_formatted_text_async(text_clean)
        logger.debug(f"JSONè§£ææˆåŠŸï¼Œè§£æç»“æœç±»å‹: {type(data)}")
        if isinstance(data, list):
            logger.debug(f"è§£æç»“æœæ˜¯åˆ—è¡¨ï¼Œé•¿åº¦: {len(data)}")
            if len(data) > 0:
                logger.debug(f"ç¬¬ä¸€ä¸ªå…ƒç´ ç±»å‹: {type(data[0])}")
                logger.debug(f"ç¬¬ä¸€ä¸ªå…ƒç´ : {data[0]}")
        elif isinstance(data, dict):
            logger.debug(f"è§£æç»“æœæ˜¯å­—å…¸ï¼Œé”®: {list(data.keys())}")
        elif isinstance(data, str):
            logger.debug(f"è§£æç»“æœä»ç„¶æ˜¯å­—ç¬¦ä¸²ï¼Œé•¿åº¦: {len(data)}")
            logger.debug(f"å­—ç¬¦ä¸²å†…å®¹å‰100å­—ç¬¦: {data[:100]}...")
            # å¦‚æœè¿˜æ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•å†æ¬¡è§£æ
            try:
                data = json.loads(data)
                logger.debug(f"äºŒæ¬¡JSONè§£ææˆåŠŸï¼Œç»“æœç±»å‹: {type(data)}")
            except Exception as e2:
                logger.error(f"äºŒæ¬¡JSONè§£æå¤±è´¥: {e2}")
        else:
            logger.debug(f"è§£æç»“æœæ˜¯å…¶ä»–ç±»å‹: {type(data)}, å€¼: {data}")
    except Exception as e:
        logger.error(f"JSONè§£æå¤±è´¥: {e}")
        raise ValueError(f"ç¿»è¯‘ç»“æœä¸æ˜¯åˆæ³•JSON: {e}\n{text_translate}")

    # ç¡®ä¿dataæ˜¯åˆ—è¡¨
    if not isinstance(data, list):
        logger.error(f"æœŸæœ›è§£æç»“æœä¸ºåˆ—è¡¨ï¼Œä½†å¾—åˆ°: {type(data)}")
        raise ValueError(f"ç¿»è¯‘ç»“æœè§£æåä¸æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œè€Œæ˜¯: {type(data)}")

    # å¤„ç†æ–°çš„JSONæ ¼å¼ï¼šå¸¦box_indexå’Œparagraph_indexçš„æ•°ç»„
    box_paragraph_translations = {}

    for i, item in enumerate(data):
        logger.debug(f"å¤„ç†ç¬¬ {i} ä¸ªå…ƒç´ ï¼Œç±»å‹: {type(item)}")

        if not isinstance(item, dict):
            logger.error(f"æ•°ç»„å…ƒç´  {i} ä¸æ˜¯å­—å…¸ç±»å‹ï¼Œè€Œæ˜¯: {type(item)}, å€¼: {item}")
            continue

        box_index = item.get("box_index")
        paragraph_index = item.get("paragraph_index")
        target_language = item.get("target_language", "")

        logger.debug(f"å…ƒç´  {i}: box_index={box_index}, paragraph_index={paragraph_index}")

        if box_index is not None and paragraph_index is not None:
            # åˆ›å»ºå¤åˆé”®ï¼šbox_index_paragraph_index
            key = f"{box_index}_{paragraph_index}"

            # å°†ç¿»è¯‘æ–‡æœ¬æŒ‰[block]åˆ†å‰²æˆç‰‡æ®µ
            fragments = [seg.strip() for seg in re.split(r"\[(?:block|å—)\]", target_language) if seg.strip()]
            box_paragraph_translations[key] = fragments

            logger.debug(f"è§£ææ–‡æœ¬æ¡† {box_index} æ®µè½ {paragraph_index}: {len(fragments)} ä¸ªç‰‡æ®µ")

    logger.info(f"è§£æåˆ° {len(box_paragraph_translations)} ä¸ªæ–‡æœ¬æ¡†æ®µè½çš„ç¿»è¯‘ç»“æœ")
    return box_paragraph_translations


def validate_page_indices(text_boxes_data):
    """
    éªŒè¯é¡µé¢ç´¢å¼•çš„æ­£ç¡®æ€§ï¼Œæ£€æµ‹å¯èƒ½çš„é¡µé¢ç´¢å¼•é‡æ–°æ˜ å°„bug
    """
    logger = get_logger("pyuno")
    page_indices = set(bp["page_index"] for bp in text_boxes_data)
    page_indices_sorted = sorted(page_indices)

    logger.info(f"æ£€æµ‹åˆ°çš„é¡µé¢ç´¢å¼•: {page_indices_sorted}")

    # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­çš„ 0,1,2... åºåˆ—ï¼ˆå¯èƒ½æ˜¯é‡æ–°æ˜ å°„bugçš„ç‰¹å¾ï¼‰
    if len(page_indices_sorted) > 1 and page_indices_sorted == list(range(len(page_indices_sorted))):
        logger.warning("âš ï¸  æ£€æµ‹åˆ°è¿ç»­çš„é¡µé¢ç´¢å¼•åºåˆ— (0,1,2,...)ï¼Œè¿™å¯èƒ½è¡¨æ˜å­˜åœ¨é¡µé¢ç´¢å¼•é‡æ–°æ˜ å°„bugï¼")
        logger.warning(
            "âš ï¸  å¦‚æœç”¨æˆ·é€‰æ‹©çš„ä¸æ˜¯è¿ç»­é¡µé¢ï¼Œè¯·æ£€æŸ¥ ppt_data_utils.py ä¸­çš„ extract_texts_for_translation å‡½æ•°"
        )
    else:
        logger.info("âœ… é¡µé¢ç´¢å¼•çœ‹èµ·æ¥æ­£ç¡®ï¼ˆä¸æ˜¯ç®€å•çš„é‡æ–°æ˜ å°„åºåˆ—ï¼‰")

    return page_indices


def validate_page_indices(text_boxes_data):
    """
    éªŒè¯é¡µé¢ç´¢å¼•çš„æ­£ç¡®æ€§ï¼Œæ£€æµ‹å¯èƒ½çš„é¡µé¢ç´¢å¼•é‡æ–°æ˜ å°„bug
    """
    logger = get_logger("pyuno")
    page_indices = set(bp["page_index"] for bp in text_boxes_data)
    page_indices_sorted = sorted(page_indices)

    logger.info(f"æ£€æµ‹åˆ°çš„é¡µé¢ç´¢å¼•: {page_indices_sorted}")

    # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­çš„ 0,1,2... åºåˆ—ï¼ˆå¯èƒ½æ˜¯é‡æ–°æ˜ å°„bugçš„ç‰¹å¾ï¼‰
    if len(page_indices_sorted) > 1 and page_indices_sorted == list(range(len(page_indices_sorted))):
        logger.warning("âš ï¸  æ£€æµ‹åˆ°è¿ç»­çš„é¡µé¢ç´¢å¼•åºåˆ— (0,1,2,...)ï¼Œè¿™å¯èƒ½è¡¨æ˜å­˜åœ¨é¡µé¢ç´¢å¼•é‡æ–°æ˜ å°„bugï¼")
        logger.warning(
            "âš ï¸  å¦‚æœç”¨æˆ·é€‰æ‹©çš„ä¸æ˜¯è¿ç»­é¡µé¢ï¼Œè¯·æ£€æŸ¥ ppt_data_utils.py ä¸­çš„ extract_texts_for_translation å‡½æ•°"
        )
    else:
        logger.info("âœ… é¡µé¢ç´¢å¼•çœ‹èµ·æ¥æ­£ç¡®ï¼ˆä¸æ˜¯ç®€å•çš„é‡æ–°æ˜ å°„åºåˆ—ï¼‰")

    return page_indices


def format_page_text_for_translation(text_boxes_data, page_index):
    """
    æ ¼å¼åŒ–æŒ‡å®šé¡µé¢çš„æ–‡æœ¬ç”¨äºç¿»è¯‘APIè°ƒç”¨ï¼ˆæ”¯æŒæ®µè½å±‚çº§ï¼‰
    âœ… ä¿®å¤ç‰ˆæœ¬ï¼šå¢å¼ºé¡µé¢ç´¢å¼•éªŒè¯å’Œæ—¥å¿—

    Args:
        text_boxes_data: æ–‡æœ¬æ¡†æ®µè½æ•°æ®åˆ—è¡¨
        page_index: è¦å¤„ç†çš„é¡µé¢ç´¢å¼•ï¼ˆPPTä¸­çš„çœŸå®é¡µé¢ç´¢å¼•ï¼‰

    Returns:
        str: æ ¼å¼åŒ–åçš„é¡µé¢æ–‡æœ¬å†…å®¹
    """
    # è¿‡æ»¤å‡ºæŒ‡å®šé¡µé¢çš„æ–‡æœ¬æ¡†æ®µè½æ•°æ®
    page_box_paragraphs = [box_para for box_para in text_boxes_data if box_para["page_index"] == page_index]

    if not page_box_paragraphs:
        logger.warning(f"âš ï¸  é¡µé¢ç´¢å¼• {page_index} æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„æ–‡æœ¬æ¡†æ®µè½æ•°æ®")
        return ""

    # âœ… æ›´æ¸…æ™°çš„é¡µé¢æ ‡è¯†ï¼Œæ˜ç¡®æ˜¾ç¤ºè¿™æ˜¯PPTä¸­çš„çœŸå®é¡µé¢ç´¢å¼•
    formatted_text = f"ç¬¬{page_index + 1}é¡µå†…å®¹ï¼ˆPPTåŸå§‹é¡µé¢ç´¢å¼•ï¼š{page_index}ï¼‰ï¼š\n\n"

    # æŒ‰æ–‡æœ¬æ¡†å’Œæ®µè½ç»„ç»‡æ•°æ®
    box_paragraphs_dict = {}
    for box_para in page_box_paragraphs:
        box_index = box_para["box_index"]
        paragraph_index = box_para["paragraph_index"]

        if box_index not in box_paragraphs_dict:
            box_paragraphs_dict[box_index] = {}

        box_paragraphs_dict[box_index][paragraph_index] = box_para

    # æŒ‰æ–‡æœ¬æ¡†ç´¢å¼•æ’åºè¾“å‡º
    for box_index in sorted(box_paragraphs_dict.keys()):
        paragraphs_dict = box_paragraphs_dict[box_index]

        # æŒ‰æ®µè½ç´¢å¼•æ’åºè¾“å‡º
        for paragraph_index in sorted(paragraphs_dict.keys()):
            box_para = paragraphs_dict[paragraph_index]

            # ä½¿ç”¨1-basedç´¢å¼•æ˜¾ç¤º
            formatted_text += f"ã€æ–‡æœ¬æ¡†{box_index + 1}-æ®µè½{paragraph_index + 1}ã€‘\n"
            formatted_text += f"{box_para['combined_text']}\n\n"

    logger.debug(f"PPTç¬¬ {page_index + 1} é¡µï¼ˆåŸå§‹ç´¢å¼•{page_index}ï¼‰æ ¼å¼åŒ–äº† {len(page_box_paragraphs)} ä¸ªæ–‡æœ¬æ¡†æ®µè½")
    return formatted_text.strip()


def translate_pages_by_page(
    text_boxes_data, progress_callback, source_language, target_language, model, stop_words_list, custom_translations
):
    """
    æŒ‰é¡µç¿»è¯‘æ–‡æœ¬å†…å®¹ï¼Œæ¯é¡µè°ƒç”¨ä¸€æ¬¡ç¿»è¯‘APIï¼ˆæ”¯æŒæ®µè½å±‚çº§ï¼‰
    âœ… ä¿®å¤ç‰ˆæœ¬ï¼šæ­£ç¡®å¤„ç†é¡µé¢ç´¢å¼•å’Œè¿›åº¦å›è°ƒ

    Args:
        text_boxes_data: æ–‡æœ¬æ¡†æ®µè½æ•°æ®åˆ—è¡¨
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        source_language: æºè¯­è¨€
        target_language: ç›®æ ‡è¯­è¨€
        model: ä½¿ç”¨çš„ç¿»è¯‘æ¨¡å‹

    Returns:
        dict: ç¿»è¯‘ç»“æœï¼Œæ ¼å¼ä¸º {page_index: translated_content}
    """
    logger.info(f"å¼€å§‹æŒ‰é¡µç¿»è¯‘ï¼ˆæ®µè½å±‚çº§ï¼‰ï¼Œå…± {len(text_boxes_data)} ä¸ªæ–‡æœ¬æ¡†æ®µè½")

    # âœ… æ–°å¢ï¼šéªŒè¯é¡µé¢ç´¢å¼•çš„æ­£ç¡®æ€§
    page_indices = validate_page_indices(text_boxes_data)
    page_indices_sorted = sorted(page_indices)
    total_pages = len(page_indices_sorted)

    logger.info(f"éœ€è¦ç¿»è¯‘çš„é¡µé¢ç´¢å¼•: {page_indices_sorted}")
    logger.info(f"æ€»å…±éœ€è¦ç¿»è¯‘ {total_pages} é¡µ")

    # âœ… å¢å¼ºï¼šæ˜¾ç¤ºæ¯é¡µçš„è¯¦ç»†ç»Ÿè®¡ï¼ŒéªŒè¯é¡µé¢ç´¢å¼•æ­£ç¡®æ€§
    logger.info("=" * 50)
    logger.info("å„é¡µé¢æ–‡æœ¬æ¡†æ®µè½åˆ†å¸ƒéªŒè¯:")
    for page_index in page_indices_sorted:
        page_box_paragraphs = [bp for bp in text_boxes_data if bp["page_index"] == page_index]
        logger.info(f"PPTç¬¬ {page_index + 1} é¡µï¼ˆåŸå§‹ç´¢å¼•{page_index}ï¼‰: {len(page_box_paragraphs)} ä¸ªæ–‡æœ¬æ¡†æ®µè½")

        # æ˜¾ç¤ºè¯¦ç»†çš„æ–‡æœ¬æ¡†æ®µè½åˆ†å¸ƒ
        box_para_dist = {}
        for bp in page_box_paragraphs:
            box_idx = bp["box_index"]
            if box_idx not in box_para_dist:
                box_para_dist[box_idx] = 0
            box_para_dist[box_idx] += 1

        for box_idx in sorted(box_para_dist.keys()):
            logger.info(f"    æ–‡æœ¬æ¡† {box_idx + 1}: {box_para_dist[box_idx]} ä¸ªæ®µè½")
    logger.info("=" * 50)

    translation_results = {}

    # åˆå§‹åŒ–è¿›åº¦å›è°ƒ
    if progress_callback:
        progress_callback(0, total_pages)

    # âœ… ä¿®å¤ï¼šä½¿ç”¨æšä¸¾æ¥è·å–æ­£ç¡®çš„è¿›åº¦åºå·ï¼ŒåŒæ—¶ä¿æŒçœŸå®çš„é¡µé¢ç´¢å¼•
    for current_page_number, page_index in enumerate(page_indices_sorted, 1):
        logger.info("=" * 60)
        logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {current_page_number}/{total_pages} é¡µ")
        logger.info(f"å¯¹åº”PPTç¬¬ {page_index + 1} é¡µï¼ˆåŸå§‹é¡µé¢ç´¢å¼•ï¼š{page_index}ï¼‰")
        logger.info("=" * 60)

        # âœ… ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å½“å‰é¡µé¢æ•°è¿›è¡Œè¿›åº¦å›è°ƒ
        if progress_callback:
            progress_callback(current_page_number - 1, total_pages)

        # ç”Ÿæˆè¯¥é¡µçš„æ ¼å¼åŒ–æ–‡æœ¬
        page_content = format_page_text_for_translation(text_boxes_data, page_index)

        if not page_content:
            logger.warning(f"PPTç¬¬ {page_index + 1} é¡µï¼ˆåŸå§‹ç´¢å¼•{page_index}ï¼‰æ²¡æœ‰æ–‡æœ¬å†…å®¹ï¼Œè·³è¿‡")
            continue

        logger.info(f"PPTç¬¬ {page_index + 1} é¡µæ ¼å¼åŒ–å®Œæˆ:")
        logger.info(f"  æ ¼å¼åŒ–æ–‡æœ¬é•¿åº¦: {len(page_content)} å­—ç¬¦")
        logger.info("-" * 40)
        # logger.info(page_content)  # å¯ä»¥å–æ¶ˆæ³¨é‡ŠæŸ¥çœ‹è¯¦ç»†å†…å®¹
        logger.info("-" * 40)

        try:
            # è°ƒç”¨ç¿»è¯‘API
            logger.info(f"æ­£åœ¨è°ƒç”¨ç¿»è¯‘APIç¿»è¯‘PPTç¬¬ {page_index + 1} é¡µ...")
            translated_result = translate(
                page_content,
                model=model,
                stop_words=stop_words_list,
                custom_translations=custom_translations,
                source_language=source_language,
                target_language=target_language,
            )
            logger.info(f"PPTç¬¬ {page_index + 1} é¡µç¿»è¯‘å®Œæˆ")

            logger.info("ç¿»è¯‘ç»“æœ:")
            logger.info(f"  ç¿»è¯‘ç»“æœé•¿åº¦: {len(translated_result)} å­—ç¬¦")
            logger.info("-" * 40)
            logger.info(translated_result)  # å¯ä»¥å–æ¶ˆæ³¨é‡ŠæŸ¥çœ‹è¯¦ç»†å†…å®¹
            logger.info("-" * 40)

            # è§£æç¿»è¯‘ç»“æœ
            translated_fragments = separate_translate_text(translated_result)

            # å­˜å‚¨ç¿»è¯‘ç»“æœ - ä½¿ç”¨çœŸå®çš„é¡µé¢ç´¢å¼•ä½œä¸ºé”®
            page_box_paragraphs = [bp for bp in text_boxes_data if bp["page_index"] == page_index]

            translation_results[page_index] = {  # âœ… ä½¿ç”¨çœŸå®çš„é¡µé¢ç´¢å¼•ä½œä¸ºé”®
                "original_content": page_content,
                "translated_json": translated_result,
                "translated_fragments": translated_fragments,
                "box_paragraph_count": len(page_box_paragraphs),
                "box_count": len(set(bp["box_index"] for bp in page_box_paragraphs)),
                "ppt_page_number": page_index + 1,  # PPTä¸­çš„æ˜¾ç¤ºé¡µç 
                "processing_sequence": current_page_number,  # å¤„ç†åºå·
                "original_page_index": page_index,  # åŸå§‹é¡µé¢ç´¢å¼•
            }

            logger.info(f"PPTç¬¬ {page_index + 1} é¡µç¿»è¯‘å®Œæˆï¼Œå¾—åˆ° {len(translated_fragments)} ä¸ªæ–‡æœ¬æ¡†æ®µè½çš„ç¿»è¯‘")

            # æ˜¾ç¤ºç¿»è¯‘ç»“æœçš„é”®å€¼å¯¹åº”å…³ç³»
            logger.info("ç¿»è¯‘ç»“æœé”®å€¼æ˜ å°„:")
            for key, fragments in translated_fragments.items():
                logger.info(f"    {key}: {len(fragments)} ä¸ªç‰‡æ®µ")

        except Exception as e:
            logger.error(f"ç¿»è¯‘PPTç¬¬ {page_index + 1} é¡µæ—¶å‡ºé”™: {e}", exc_info=True)
            # å¦‚æœç¿»è¯‘å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä¿¡æ¯
            page_box_paragraphs = [bp for bp in text_boxes_data if bp["page_index"] == page_index]
            translation_results[page_index] = {
                "original_content": page_content,
                "error": str(e),
                "translated_fragments": {},
                "box_paragraph_count": len(page_box_paragraphs),
                "box_count": len(set(bp["box_index"] for bp in page_box_paragraphs)),
                "ppt_page_number": page_index + 1,
                "processing_sequence": current_page_number,
                "original_page_index": page_index,
            }

    # å®Œæˆè¿›åº¦å›è°ƒ
    if progress_callback:
        progress_callback(total_pages, total_pages)

    logger.info("=" * 60)
    logger.info(f"æŒ‰é¡µç¿»è¯‘å®Œæˆï¼Œå…±å¤„ç† {len(translation_results)} é¡µ")
    logger.info("=" * 60)

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    successful_pages = len([r for r in translation_results.values() if "error" not in r])
    failed_pages = len([r for r in translation_results.values() if "error" in r])
    total_box_paragraphs_translated = sum(len(r.get("translated_fragments", {})) for r in translation_results.values())
    total_boxes_translated = sum(r.get("box_count", 0) for r in translation_results.values())

    logger.info("ç¿»è¯‘ç»Ÿè®¡:")
    logger.info(f"  - æˆåŠŸç¿»è¯‘é¡µæ•°: {successful_pages}")
    logger.info(f"  - ç¿»è¯‘å¤±è´¥é¡µæ•°: {failed_pages}")
    logger.info(f"  - æ€»ç¿»è¯‘æ–‡æœ¬æ¡†æ•°: {total_boxes_translated}")
    logger.info(f"  - æ€»ç¿»è¯‘æ–‡æœ¬æ¡†æ®µè½æ•°: {total_box_paragraphs_translated}")

    # âœ… å¢å¼ºï¼šæ˜¾ç¤ºè¯¦ç»†çš„é¡µé¢å¤„ç†ä¿¡æ¯ï¼ŒéªŒè¯é¡µé¢ç´¢å¼•æ˜ å°„æ­£ç¡®æ€§
    logger.info("è¯¦ç»†é¡µé¢å¤„ç†éªŒè¯:")
    for page_index, result in translation_results.items():
        ppt_page_num = result.get("ppt_page_number", page_index + 1)
        processing_seq = result.get("processing_sequence", "?")
        original_idx = result.get("original_page_index", page_index)
        status = "æˆåŠŸ" if "error" not in result else f"å¤±è´¥({result.get('error', 'unknown')[:50]}...)"
        logger.info(f"  å¤„ç†åºå· {processing_seq}: PPTç¬¬{ppt_page_num}é¡µï¼ˆåŸå§‹ç´¢å¼•{original_idx}ï¼‰- {status}")

    return translation_results


def extract_json_block(text):
    """
    å°è¯•æå–æœ€å¤–å±‚çš„[]æˆ–{}åŒ…è£¹çš„å†…å®¹
    """
    import re

    match = re.search(r"(\[.*\]|\{.*\})", text, re.DOTALL)
    if match:
        return match.group(1)
    return text  # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›åŸæ–‡


def validate_translation_result(translation_results, text_boxes_data):
    """
    éªŒè¯ç¿»è¯‘ç»“æœçš„å®Œæ•´æ€§å’Œæ­£ç¡®æ€§

    Args:
        translation_results: ç¿»è¯‘ç»“æœ
        text_boxes_data: åŸå§‹æ–‡æœ¬æ¡†æ®µè½æ•°æ®

    Returns:
        dict: éªŒè¯ç»“æœç»Ÿè®¡
    """
    logger = get_logger("pyuno")
    logger.info("å¼€å§‹éªŒè¯ç¿»è¯‘ç»“æœ...")

    validation_stats = {
        "total_expected_box_paragraphs": len(text_boxes_data),
        "total_translated_box_paragraphs": 0,
        "missing_translations": [],
        "extra_translations": [],
        "fragment_count_mismatches": [],
        "pages_processed": len(translation_results),
    }

    try:
        # åˆ›å»ºé¢„æœŸçš„æ–‡æœ¬æ¡†æ®µè½æ˜ å°„
        expected_box_paragraphs = {}
        for box_para in text_boxes_data:
            page_idx = box_para["page_index"]
            box_idx = box_para["box_index"]
            para_idx = box_para["paragraph_index"]
            key = f"{box_idx + 1}_{para_idx + 1}"  # è½¬ä¸º1-based

            if page_idx not in expected_box_paragraphs:
                expected_box_paragraphs[page_idx] = {}

            expected_box_paragraphs[page_idx][key] = {
                "expected_fragments": len(box_para["texts"]),
                "box_para_data": box_para,
            }

        # éªŒè¯æ¯é¡µçš„ç¿»è¯‘ç»“æœ
        for page_idx, translation_result in translation_results.items():
            if "error" in translation_result:
                logger.warning(f"ç¬¬ {page_idx + 1} é¡µç¿»è¯‘å¤±è´¥ï¼Œè·³è¿‡éªŒè¯")
                continue

            translated_fragments = translation_result.get("translated_fragments", {})
            expected_for_page = expected_box_paragraphs.get(page_idx, {})

            # æ£€æŸ¥ç¼ºå¤±çš„ç¿»è¯‘
            for expected_key in expected_for_page:
                if expected_key not in translated_fragments:
                    validation_stats["missing_translations"].append(f"é¡µé¢{page_idx + 1}-{expected_key}")
                else:
                    # æ£€æŸ¥ç‰‡æ®µæ•°é‡æ˜¯å¦åŒ¹é…
                    expected_count = expected_for_page[expected_key]["expected_fragments"]
                    actual_count = len(translated_fragments[expected_key])

                    if expected_count != actual_count:
                        validation_stats["fragment_count_mismatches"].append(
                            {
                                "location": f"é¡µé¢{page_idx + 1}-{expected_key}",
                                "expected": expected_count,
                                "actual": actual_count,
                            }
                        )

                    validation_stats["total_translated_box_paragraphs"] += 1

            # æ£€æŸ¥å¤šä½™çš„ç¿»è¯‘
            for actual_key in translated_fragments:
                if actual_key not in expected_for_page:
                    validation_stats["extra_translations"].append(f"é¡µé¢{page_idx + 1}-{actual_key}")

        # è®¡ç®—éªŒè¯ç»Ÿè®¡
        validation_stats["translation_coverage"] = (
            validation_stats["total_translated_box_paragraphs"]
            / validation_stats["total_expected_box_paragraphs"]
            * 100
            if validation_stats["total_expected_box_paragraphs"] > 0
            else 0
        )

        # è®°å½•éªŒè¯ç»“æœ
        logger.info("ç¿»è¯‘ç»“æœéªŒè¯å®Œæˆ:")
        logger.info(f"  - é¢„æœŸæ–‡æœ¬æ¡†æ®µè½æ•°: {validation_stats['total_expected_box_paragraphs']}")
        logger.info(f"  - å®é™…ç¿»è¯‘æ–‡æœ¬æ¡†æ®µè½æ•°: {validation_stats['total_translated_box_paragraphs']}")
        logger.info(f"  - ç¿»è¯‘è¦†ç›–ç‡: {validation_stats['translation_coverage']:.2f}%")
        logger.info(f"  - ç¼ºå¤±ç¿»è¯‘æ•°: {len(validation_stats['missing_translations'])}")
        logger.info(f"  - å¤šä½™ç¿»è¯‘æ•°: {len(validation_stats['extra_translations'])}")
        logger.info(f"  - ç‰‡æ®µæ•°é‡ä¸åŒ¹é…æ•°: {len(validation_stats['fragment_count_mismatches'])}")

        # å¦‚æœæœ‰é—®é¢˜ï¼Œè®°å½•è¯¦ç»†ä¿¡æ¯
        if validation_stats["missing_translations"]:
            logger.warning(f"ç¼ºå¤±çš„ç¿»è¯‘: {validation_stats['missing_translations']}")

        if validation_stats["extra_translations"]:
            logger.warning(f"å¤šä½™çš„ç¿»è¯‘: {validation_stats['extra_translations']}")

        if validation_stats["fragment_count_mismatches"]:
            logger.warning("ç‰‡æ®µæ•°é‡ä¸åŒ¹é…çš„æƒ…å†µ:")
            for mismatch in validation_stats["fragment_count_mismatches"]:
                logger.warning(f"  {mismatch['location']}: é¢„æœŸ {mismatch['expected']}, å®é™… {mismatch['actual']}")

        return validation_stats

    except Exception as e:
        logger.error(f"éªŒè¯ç¿»è¯‘ç»“æœæ—¶å‡ºé”™: {e}", exc_info=True)
        return validation_stats


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("=" * 60)
    print("api_translate_uno æ¨¡å—æµ‹è¯•ï¼ˆæ®µè½å±‚çº§æ”¯æŒï¼‰")
    print("=" * 60)

    logger = get_logger("pyuno.test")
    logger.info("api_translate_uno æ¨¡å—åŠ è½½æˆåŠŸ")

    # åˆ›å»ºæ¨¡æ‹Ÿçš„æ–‡æœ¬æ¡†æ®µè½æ•°æ®è¿›è¡Œæµ‹è¯•
    mock_text_boxes_data = [
        {
            "page_index": 0,
            "box_index": 0,
            "box_id": "textbox_0",
            "paragraph_index": 0,
            "paragraph_id": "para_0_0",
            "texts": ["Hello", "world"],
            "combined_text": "Hello[block]world",
        },
        {
            "page_index": 0,
            "box_index": 0,
            "box_id": "textbox_0",
            "paragraph_index": 1,
            "paragraph_id": "para_0_1",
            "texts": ["This is", "a test"],
            "combined_text": "This is[block]a test",
        },
    ]

    try:
        # æµ‹è¯•æ ¼å¼åŒ–å‡½æ•°
        formatted_text = format_page_text_for_translation(mock_text_boxes_data, 0)
        logger.info("æ ¼å¼åŒ–æµ‹è¯•æˆåŠŸ:")
        logger.info(formatted_text)

        # æµ‹è¯•ç¿»è¯‘ç»“æœè§£æï¼ˆæ¨¡æ‹Ÿï¼‰
        mock_translation_result = """[
            {
                "box_index": 1,
                "paragraph_index": 1,
                "source_language": "Hello[block]world",
                "target_language": "ä½ å¥½[block]ä¸–ç•Œ"
            },
            {
                "box_index": 1,
                "paragraph_index": 2,
                "source_language": "This is[block]a test",
                "target_language": "è¿™æ˜¯[block]ä¸€ä¸ªæµ‹è¯•"
            }
        ]"""

        translated_fragments = separate_translate_text(mock_translation_result)
        logger.info("ç¿»è¯‘ç»“æœè§£ææµ‹è¯•æˆåŠŸ:")
        for key, fragments in translated_fragments.items():
            logger.info(f"  {key}: {fragments}")

        # æµ‹è¯•åç«¯APIè°ƒç”¨ï¼ˆå¦‚æœæœ‰ç½‘ç»œè¿æ¥ï¼‰
        try:
            test_text = """ç¬¬1é¡µå†…å®¹ï¼š

ã€æ–‡æœ¬æ¡†1-æ®µè½1ã€‘
Hello[block]world

ã€æ–‡æœ¬æ¡†1-æ®µè½2ã€‘
This is[block]a test"""

            logger.info("æµ‹è¯•åç«¯APIè°ƒç”¨ (deepseek):")
            result = call_backend_translate_ppt_page(test_text, "deepseek")
            logger.info(f"åç«¯APIæµ‹è¯•æˆåŠŸï¼Œè¿”å›data: {result}")

        except Exception as e:
            logger.warning(f"åç«¯APIæµ‹è¯•å¤±è´¥ï¼ˆå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼‰: {e}")

    except Exception as e:
        logger.error(f"æµ‹è¯•å¤±è´¥: {e}", exc_info=True)

    print("=" * 60)
